from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.metrics import f1_score, precision_recall_curve, auc, roc_curve,\
                            accuracy_score, cohen_kappa_score, roc_auc_score

import numpy as np
import pandas as pd
import itertools
import pdb

from atm.constants import *


def rank_n_accuracy(y_true, y_prob_mat, rank=5):
    """
    Compute the model's accuracy on just the n most commmon classes (I think?)
    """
    rankings = np.argsort(-y_prob_mat) # negative because we want highest value first
    rankings = rankings[:, 0:rank-1]

    num_samples = len(y_true)
    correct_sample_count = 0.0

    for i in range(num_samples):
        if y_true[i] in rankings[i, :]:
            correct_sample_count += 1.0

    return correct_sample_count / num_samples


def get_pr_roc(y_true, y_pred_probs, include_pr=False, include_roc=False):
    """
    y_true: list of true class labels (only 1 or 0)
    y_pred_probs: list of probabilities generated by the model for the label
        class 1
    include_pr: if True, return full precision/recall curve data
    include_roc: if True, return full receiver operating characteristic data
    """
    results = {}
    if not np.any(np.isnan(y_pred_probs)):
        if include_roc:
            roc = roc_curve(y_true, y_pred_probs, pos_label=1)
            results['roc'] = {
                'fprs': pr[0],
                'tprs': pr[1],
                'thresholds': pr[2],
            }
            results[Metrics.ROC_AUC] = auc(roc[0], roc[1])
        else:
            results[Metrics.ROC_AUC] = roc_auc_score(roc_fprs, roc_tprs)

        if include_pr:
            pr = precision_recall_curve(y_true, y_pred_probs, pos_label=1)
            results['pr'] = {
                'precisions': pr[0],
                'recalls': pr[1],
                'thresholds': pr[2],
            }
            results[Metrics.PR_AUC] = auc(pr[1], pr[0])
        else:
            results[Metrics.PR_AUC] = np.nan

    return results


def get_metrics_binary(y_true, y_pred, y_pred_probs, include_pr=False,
                       include_roc=False):
    results = {
        Metrics.ACCURACY: accuracy_score(y_true, y_pred),
        Metrics.COHEN_KAPPA: cohen_kappa_score(y_true, y_pred),
        Metrics.F1: f1_score(y_true, y_pred),
    }
    results.update(get_pr_roc(y_true, y_pred_probs[:, 1], include_pr=include_pr,
                              include_roc=include_roc))
    return results


def get_metrics_multiclass(y_true, y_pred, y_pred_probs, include_pr=False,
                           include_roc=False, rank_accuracy=False,
                           labelwise=False):
    results = {}
    results[Metrics.ACCURACY] = accuracy_score(y_true, y_pred)
    results[Metrics.COHEN_KAPPA] = cohen_kappa_score(y_true, y_pred)

    # this parameter should only be used for datasets with high-cardinality
    # labels (lots of poosible values)
    if rank_accuracy:
        results[Metrics.RANK_ACCURACY] = rank_n_accuracy(y_true=y_true,
                                                         y_prob_mat=y_pred_probs,
                                                         rank=rank)

    labels = range(y_pred_probs.shape[1])
    y_true_bin = np.zeros(y_pred_probs.shape)
    y_pred_bin = np.zeros(y_pred_probs.shape)
    for label in labels:
        y_true_bin[:, label] = (y_true == label).astype(int)
        y_pred_bin[:, label] = (y_pred == label).astype(int)

    results[Metrics.F1_MICRO] = f1_score(y_true, y_pred, average='micro')
    results[Metrics.F1_MACRO] = f1_score(y_true, y_pred, average='macro')

    if len(np.unique(y_true)) == 1:
        print "ROC/AUC undefined: setting AUC scores to defaults."
        results[Metrics.ROC_AUC_MICRO] = \
            METRIC_DEFAULT_SCORES[Metrics.ROC_AUC_MICRO]
        results[Metrics.ROC_AUC_MACRO] = \
            METRIC_DEFAULT_SCORES[Metrics.ROC_AUC_MACRO]
    else:
        results[Metrics.ROC_AUC_MICRO] = roc_auc_score(y_true_bin, y_pred_probs,
                                                       average='micro')
        results[Metrics.ROC_AUC_MACRO] = roc_auc_score(y_true_bin, y_pred_probs,
                                                       average='macro')

    # labelwise controls whether to compute separate metrics for each posisble label
    if labelwise or include_pr or include_roc:
        results['labelwise'] = {}
        # for each label, generate F1, precision-recall, and ROC curves
        for label in labels:
            label_res = get_pr_roc(y_true_bin[:, label],
                                   y_pred_probs[:, label],
                                   include_pr=include_pr,
                                   include_roc=include_roc)
            label_res[Metrics.F1] = f1_score(y_true=y_true_bin[:, label],
                                             y_pred=y_pred_bin[:, label],
                                             pos_label=1)
            results['labelwise'][label] = label_res

    return results


def test_pipeline(pipeline, X, y, binary, **kwargs):
    if binary:
        get_metrics = get_metrics_binary
    else:
        get_metrics = get_metrics_multiclass

    # run the test data through the trained pipeline
    y_pred = pipeline.predict(X)

    # if necessary, coerce class distance scores into probability scores
    method = pipeline.steps[-1][0]
    if method in ['sgd', 'pa']:
        class_1_distance = pipeline.decision_function(X)
        class_0_distance = -class_1_distance
        # this is needed for some of the scoring methods
        y_pred_probs = np.column_stack((class_0_distance, class_1_distance))
    else:
        y_pred_probs = pipeline.predict_proba(X)

    return get_metrics(y, y_pred, y_pred_probs, **kwargs)


def cross_validate_pipeline(pipeline, X, y, binary=True,
                            n_folds=N_FOLDS_DEFAULT, **kwargs):
    """
    Compute metrics for each of `n_folds` folds of the training data in (X, y).

    pipeline: the sklearn Pipeline to train and test
    X: feature matrix
    y: series of labels corresponding to rows in X
    binary: whether the label is binary or multi-ary
    """
    if binary:
        metrics = METRICS_BINARY
    else:
        metrics = METRICS_MULTICLASS

    df = pd.DataFrame(columns=metrics)
    results = []

    skf = StratifiedKFold(n_splits=n_folds)
    skf.get_n_splits(X, y)

    for train_index, test_index in skf.split(X, y):
        pipeline.fit(X[train_index], y[train_index])
        split_results = test_pipeline(pipeline=pipeline,
                                      X=X[test_index],
                                      y=y[test_index],
                                      binary=binary, **kwargs)
        df.append([{m: split_results.get(m) for m in metrics}])
        results.append(split_results)

    return df, results

