#################################
#   BTB CONFIGURATION FILE   #
#################################
#
# In order to use this, you must change the following fields at the very least:
# [data]
# - alldatapath
# [datahub]
# - username
# - password
#
# The most important values for tuning are:
# [run]
# - algorithms
# [budget]
# - learner

[data]
# Path to a file with a list of datafiles. All will be uploaded to the ModelHub
# using the parameters specified below.
data-filelist:
# Path to CSV file that contains all data (train & test). Must be fully
# qualified; i.e. no tildes. This is ignored if data-filelist non-blank.
alldatapath: /path/to/all/data
# Path to CSV file that contains the train data
# Ignored if data-filelist or alldatapath non-blank
trainpath: 
# Path to CSV file that contains the test data 
# Ignored if data-filelist or alldatapath non-blank
testpath: 
# Description of the data and classification task (limit 1000 characters)
data-description:




[mode]
# btb run mode (cloud or local)
run-mode: local



[datahub]
# Database dialect
dialect: sqlite
# Name of the database
database: btb.db
# Username to gain access to the database
username: 
# Password to gain access to the database
password:
# Host name of the device hosting the database
host: 
# Port on host listening for database connections
port:
# Optional field for specifying login details
query: 



[run]
# Add here the algorithm codes you'd like to run and compare. Look these up in 
# the 'algorithms' tables in the database, or alternatively, in the 
# config/hyperbtb.sql file in this repository. You must spell them correctly!
# 
# Add each algorithm code as a text string separated by a comma.
# 
# Notes:
#   - SVMs (classify_svm) can take a long time to train. It's not an error. 
#     It's justpart of what happens  when the algorithm happens to explore a 
#     crappy set of parameters on a powerful algo like this.
#   - SGDs (classify_sgd) can sometimes fail on certain parameter settings as 
#     well. Don't worry, they train SUPER fast, and the worker.py will simply 
#     log the error and continue.
#
# Algorithm Options: 
#   classify_logreg - logistic regression
#   classify_svm    - support vector machine
#   classify_sgd    - linear classifier (SVM or logreg) using stochastic
#                     gradient descent 
#   classify_dt     - decision tree
#   classify_et     - extra trees
#   classify_rf     - random forest
#   classify_gnb    - gaussian naive bayes
#   classify_mnb    - multinomial naive bayes
#   classify_bnb    - bernoulli naive bayes
#   classify_gp     - gaussian process
#   classify_pa     - passive aggressive
#   classify_knn    - K nearest neighbors
#   classify_dbn    - deep belief network
#   classify_mlp    - multi-layer perceptron
algorithms: classify_rf, classify_logreg, classify_dt
# directory to store trained models; will be created if it doesn't exist
models-dir: ./models
# not sure this is used
drop-values:
# not sure this is used
test-ratio: 
# priority (higher number is more important)
priority: 1



[budget]
# Should there be a learner or walltime budget?
budget-type: learner
# If budget-type is learner, how many learners to try?
learner-budget: 20
# If budget-type is walltime, how long to search (in minutes)?
walltime-budget: 5



[strategy]
# How should BTB sample a hyperpartition (frozen set) that it must explore?
# 	- uniform: pick randomly! (baseline)
# 	- gp_ei: Gaussian Process expected improvement criterion
# 	- gp_eitime: Gaussian Process expected improvement criterion per unit time
# 	- gp_eivel: GPEI criterion per unit velocity
# 	- grid: grid selection
sample_selection: grid
# r_min is the number of random runs performed in each hyperpartition before 
# allowing bayesian opt to select parameters. Consult the thesis to understand
# what those mean, but essentially: 
# 
# 	if (num_learners_trained_in_hyperpartition >= r_min)
# 		# train using sample criteria 
# 	else
# 		# train using uniform (baseline)
r_min: 2
# How should BTB select a particular hyperpartition (frozen set) from the 
# set of all hyperpartitions? 
# options: uniform, ucb1, best_k, best_k_vel, recent_k, recent_k_vel, hier_alg,
#          hier_rand, pure_best_k_vel
# Again, each is a different method, consult the thesis.
frozen_selection: uniform
# k is number that xxx_k methods use. It is similar to r_min, except it is 
# called k_window and determines how much "history" BTB considers for certain
# frozen selection logics.
k_window: 5
# Which field to use for judgment of performance
# options: f1, roc_auc, accuracy
metric: f1
# Which data to use for computing judgment score
# cv_judgment_metric = Cross-Validated performance on training data
# test_judgment_metric = Performance on test data
score_target: cv_judgment_metric



[git]
# git username
username: 
# git password
password:
# BTB git location (http)
repo: 



[aws]
# AWS access key
access_key: 
# AWS secret key
secret_key: 
# Number of AWS instances to start
num_instances: 
# Number of BTB workers per instances
num_workers_per_instance: 
# AWS S3 bucket to store data
s3_bucket:
# Folder in AWS S3 bucket in which to store data
s3_folder:
# Region to start instances in
ec2_region: 
# Name of BTB AMI
ec2_amis: 
# AWS key pair to use for EC2 instances
ec2_key_pair: 
# Local path to key file (must match ec2_key_pair)
ec2_keyfile: 
# Type of EC2 instance to start
ec2_instance_type: 
# Username to log into EC2 instance
ec2_username: 
